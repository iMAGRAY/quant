#!/usr/bin/env python
"""convert_qwen3_reranker_to_onnx.py

–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç –º–æ–¥–µ–ª—å `Qwen/Qwen3-Reranker-0.6B` –≤ —Ñ–æ—Ä–º–∞—Ç ONNX –∏ –≤—ã–ø–æ–ª–Ω—è–µ—Ç
–¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ –≤–µ—Å–æ–≤ –¥–æ INT8, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–Ω–∏–∑–∏—Ç—å
–æ–±—ä—ë–º —Ñ–∞–π–ª–∞ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏ –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞.

–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:

    python convert_qwen3_reranker_to_onnx.py \
        --model-id Qwen/Qwen3-Reranker-0.6B \
        --output-dir onnx/qwen3_reranker

–î–ª—è –º–æ–¥–µ–ª–µ–π >2 –ì–∏–ë –¥–æ–±–∞–≤—å—Ç–µ —Ñ–ª–∞–≥ `--external-data-format`, —á—Ç–æ–±—ã ONNX –±—ã–ª
—Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤–æ ¬´–≤–Ω–µ—à–Ω–µ–º¬ª —Ñ–æ—Ä–º–∞—Ç–µ –¥–∞–Ω–Ω—ã—Ö.
"""
from __future__ import annotations

import argparse
import logging
import sys
from pathlib import Path

from transformers import AutoTokenizer

try:
    # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å: optimum + onnxruntime
    from optimum.onnxruntime import (
        ORTModelForSequenceClassification,
        ORTQuantizer,
        QuantizationConfig,
    )
except ImportError as err:  # pragma: no cover
    raise SystemExit(
        "‚ùå  –ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ `optimum` –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –∫–æ–º–∞–Ω–¥–æ–π:\n"
        "    pip install --upgrade \"optimum[onnxruntime,export]\""
    ) from err

logging.basicConfig(
    format="[%(levelname)s] %(message)s", level=logging.INFO, stream=sys.stdout
)
LOGGER = logging.getLogger(__name__)

DEFAULT_MODEL_ID = "Qwen/Qwen3-Reranker-0.6B"
DEFAULT_OUTPUT_DIR = "onnx/qwen3_reranker"


def export_and_quantize(
    model_id: str = DEFAULT_MODEL_ID,
    output_dir: str | Path = DEFAULT_OUTPUT_DIR,
    *,
    use_external_data_format: bool = False,
    dynamic: bool = True,
) -> None:
    """–≠–∫—Å–ø–æ—Ä—Ç –∏ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏.

    Parameters
    ----------
    model_id
        HuggingFace-–∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –º–æ–¥–µ–ª–∏ –∏–ª–∏ –ø—É—Ç—å –∫ –ª–æ–∫–∞–ª—å–Ω–æ–π –ø–∞–ø–∫–µ.
    output_dir
        –ü–∞–ø–∫–∞ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤ ONNX.
    use_external_data_format
        –°–æ—Ö—Ä–∞–Ω—è—Ç—å –ª–∏ –º–æ–¥–µ–ª—å ONNX –≤–æ ¬´–≤–Ω–µ—à–Ω–µ–º¬ª —Ñ–æ—Ä–º–∞—Ç–µ –¥–∞–Ω–Ω—ã—Ö (>2 –ì–∏–ë).
    dynamic
        –í—ã–ø–æ–ª–Ω—è—Ç—å –ª–∏ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ INT8-–∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é ‚Äë –¥–∞).
    """
    output_dir = Path(output_dir).expanduser().resolve()
    output_dir.mkdir(parents=True, exist_ok=True)

    LOGGER.info("üöÄ –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ–º %s ‚Üí ONNX‚Ä¶", model_id)

    # –ü–∞—Ä–∞–º–µ—Ç—Ä `use_external_data_format` –ø–æ—è–≤–∏–ª—Å—è –≤ –Ω–µ–¥–∞–≤–Ω–∏—Ö –≤–µ—Ä—Å–∏—è—Ö `optimum`.
    # –ß—Ç–æ–±—ã –æ—Å—Ç–∞–≤–∞—Ç—å—Å—è —Å–æ–≤–º–µ—Å—Ç–∏–º—ã–º–∏ —Å –±–æ–ª–µ–µ —Å—Ç–∞—Ä—ã–º–∏ –≤–µ—Ä—Å–∏—è–º–∏, –ø—Ä–æ–≤–µ—Ä—è–µ–º
    # –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è –ª–∏ –æ–Ω —Ä–µ—Ñ–ª–µ–∫—Å–∏–≤–Ω–æ. –ï—Å–ª–∏ –Ω–µ—Ç ‚Äî –≤—ã–∑—ã–≤–∞–µ–º –±–µ–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞.
    import inspect  # –ª–æ–∫–∞–ª—å–Ω—ã–π –∏–º–ø–æ—Ä—Ç, —á—Ç–æ–±—ã –Ω–µ –∑–∞—Å–æ—Ä—è—Ç—å –≥–ª–æ–±–∞–ª—å–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ

    _fp_sig = inspect.signature(ORTModelForSequenceClassification.from_pretrained)
    kwargs: dict[str, object] = {
        "model_id_or_path": model_id,  # –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–π –±—É–¥–µ—Ç –ø–µ—Ä–µ–¥–∞–Ω –∫–∞–∫ *args
        "export": True,
    }
    if "use_external_data_format" in _fp_sig.parameters:
        kwargs["use_external_data_format"] = use_external_data_format
    else:
        if use_external_data_format:
            LOGGER.warning(
                "–í–µ—Ä—Å–∏—è optimum –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç `use_external_data_format`; "
                "ONNX –±—É–¥–µ—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ –æ–±—ã—á–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ. –û–±–Ω–æ–≤–∏—Ç–µ –ø–∞–∫–µ—Ç: "
                "pip install -U 'optimum[onnxruntime,export]'"
            )

    # –†–∞–∑–≤–æ—Ä–∞—á–∏–≤–∞–µ–º kwargs –∞–∫–∫—É—Ä–∞—Ç–Ω–æ (–±–µ–∑ –ª–∏—à–Ω–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤)
    ort_model = ORTModelForSequenceClassification.from_pretrained(
        kwargs.pop("model_id_or_path"),
        **kwargs,
    )
    tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)

    ort_model.save_pretrained(output_dir)
    tokenizer.save_pretrained(output_dir)
    LOGGER.info("‚úÖ ONNX-–º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ %s", output_dir)

    if dynamic:
        LOGGER.info("‚öôÔ∏è  –î–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ INT8-–∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ‚Ä¶")
        try:
            qconfig = QuantizationConfig.for_dynamic()  # type: ignore[attr-defined]
            quantizer = ORTQuantizer.from_pretrained(ort_model)
            quantizer.quantize(
                save_dir=output_dir,
                quantization_config=qconfig,
            )
            LOGGER.info("‚úÖ –ö–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ %s", output_dir)
        except AttributeError:
            # –°—Ç–∞—Ä—ã–µ –≤–µ—Ä—Å–∏–∏ optimum (<1.18) –Ω–µ –∏–º–µ—é—Ç –º–µ—Ç–æ–¥–∞ for_dynamic.
            LOGGER.warning(
                "QuantizationConfig.for_dynamic –Ω–µ –Ω–∞–π–¥–µ–Ω ‚Äì –ø–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ "
                "onnxruntime.quantization.quantize_dynamic. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è "
                "–æ–±–Ω–æ–≤–∏—Ç—å optimum –¥–æ –ø–æ—Å–ª–µ–¥–Ω–µ–π –≤–µ—Ä—Å–∏–∏."
            )

            from onnxruntime.quantization import QuantType, quantize_dynamic

            onnx_model_path = next(output_dir.glob("*.onnx"))
            quant_model_path = output_dir / "model.int8.onnx"

            quantize_dynamic(
                model_input=onnx_model_path.as_posix(),
                model_output=quant_model_path.as_posix(),
                weight_type=QuantType.QInt8,
                per_channel=False,
                op_types_to_quantize=["MatMul", "Gemm"],
                reduce_range=True,
            )
            LOGGER.info("‚úÖ –ö–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ %s", quant_model_path)

    LOGGER.info("üéâ –ì–æ—Ç–æ–≤–æ!")


def _parse_args() -> argparse.Namespace:  # pragma: no cover
    parser = argparse.ArgumentParser(
        description=(
            "–≠–∫—Å–ø–æ—Ä—Ç Qwen3-Reranker-0.6B –≤ ONNX c –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–º INT8-–∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ–º"
        )
    )
    parser.add_argument(
        "--model-id",
        default=DEFAULT_MODEL_ID,
        help="HF-–∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –∏–ª–∏ –ø—É—Ç—å (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: %(default)s)",
    )
    parser.add_argument(
        "--output-dir",
        default=DEFAULT_OUTPUT_DIR,
        help="–ü–∞–ø–∫–∞ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: %(default)s)",
    )
    parser.add_argument(
        "--external-data-format",
        action="store_true",
        help="–°–æ—Ö—Ä–∞–Ω—è—Ç—å ONNX –≤–æ –≤–Ω–µ—à–Ω–µ–º —Ñ–æ—Ä–º–∞—Ç–µ –¥–∞–Ω–Ω—ã—Ö (>2 –ì–∏–ë)",
    )
    parser.add_argument(
        "--no-dynamic",
        action="store_true",
        help="–û—Ç–∫–ª—é—á–∏—Ç—å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ",
    )
    return parser.parse_args()


def main() -> None:  # pragma: no cover
    args = _parse_args()
    export_and_quantize(
        model_id=args.model_id,
        output_dir=args.output_dir,
        use_external_data_format=args.external_data_format,
        dynamic=not args.no_dynamic,
    )


if __name__ == "__main__":  # pragma: no cover
    main()